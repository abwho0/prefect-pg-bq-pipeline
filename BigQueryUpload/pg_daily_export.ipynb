{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import gspread\n",
    "import warnings\n",
    "import csv\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from google.oauth2.service_account import Credentials\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_gsutil():\n",
    "    \"\"\"Checks if gsutil is installed and in the PATH.\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"gsutil\", \"--version\"], check=True, capture_output=True, text=True)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "check_gsutil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Table 'oltp_payments' created successfully.\n",
      "✅ Table 'oltp_payments_temp' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Setup your credentials and client\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/pg-bigquery-pipeline.json\"\n",
    ")\n",
    "client = bigquery.Client(credentials=credentials, project=\"ajar-kw\")\n",
    "\n",
    "def create_bigquery_tables(client):\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"id\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"reference_id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"internal_id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"broker_id\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"contact_id\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"user_id\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"payment_method_id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"currency\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"offline\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"extra\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"amount\", \"NUMERIC\"),\n",
    "        bigquery.SchemaField(\"refunded_amount\", \"NUMERIC\"),\n",
    "        bigquery.SchemaField(\"status\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"card_bin_id\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"ip_address\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"by_payment_link\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"captured_at\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"created_at\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"updated_at\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"archived_at\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"created_by\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"updated_by\", \"INTEGER\"),\n",
    "    ]\n",
    "    \n",
    "    BIGQUERY_PROJECT_ID = \"ajar-kw\"\n",
    "    BIGQUERY_DATASET_NAME = \"payments_data\"\n",
    "\n",
    "    table_names = [\"oltp_payments\", \"oltp_payments_temp\"]\n",
    "\n",
    "    for table_name in table_names:\n",
    "        table_id = f\"{BIGQUERY_PROJECT_ID}.{BIGQUERY_DATASET_NAME}.{table_name}\"\n",
    "        table = bigquery.Table(table_id)\n",
    "        table.schema = schema\n",
    "        table.time_partitioning = bigquery.TimePartitioning(field=\"updated_at\")\n",
    "        try:\n",
    "            client.create_table(table)\n",
    "            print(f\"✅ Table '{table_name}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            if \"Already Exists\" in str(e):\n",
    "                print(f\"ℹ️ Table '{table_name}' already exists — skipping creation.\")\n",
    "            else:\n",
    "                print(f\"❌ Error creating table '{table_name}': {e}\")\n",
    "\n",
    "# Call it with the connected client\n",
    "create_bigquery_tables(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected successfully. Found datasets:\n",
      " - dbt_aajmal\n",
      " - dev_dbt\n",
      " - looker_pdt_scratch\n",
      " - payments_data\n",
      " - raw_accounts\n",
      " - staging_dbt\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/pg-bigquery-pipeline.json\"\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=\"ajar-kw\")\n",
    "\n",
    "# Try listing datasets\n",
    "datasets = list(client.list_datasets())\n",
    "if datasets:\n",
    "    print(\"✅ Connected successfully. Found datasets:\")\n",
    "    for ds in datasets:\n",
    "        print(f\" - {ds.dataset_id}\")\n",
    "else:\n",
    "    print(\"✅ Connected successfully, but no datasets found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data pipeline for 2025-06-26...\n",
      "Successfully exported 3051 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/yesterday_payments.csv\n",
      "Successfully uploaded /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/yesterday_payments.csv to gs://ajar-bigquery-staging-bucket/raw/20250626_yesterday_payments.csv\n",
      "Successfully loaded data into BigQuery: \n",
      "Successfully deleted gs://ajar-bigquery-staging-bucket/raw/20250626_yesterday_payments.csv from GCS\n",
      "Successfully deleted file from GCS.\n",
      "Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Configuration (Hardcoded with your values - BEST PRACTICE: use environment variables)\n",
    "DB_HOST = \"34.93.7.102\"\n",
    "DB_NAME = \"ajar\"\n",
    "DB_USER = \"tech\"\n",
    "DB_PASSWORD = \">aRSIeB(C,gHuo1|\"\n",
    "GCS_BUCKET_NAME = \"ajar-bigquery-staging-bucket\"\n",
    "BIGQUERY_DATASET_NAME = \"ajar-kw:payments_data\"  # Include the project ID here.\n",
    "BIGQUERY_TABLE_NAME = \"yesterday_payments\"  #  No date suffix here, we'll add in the code.\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/yesterday_payments.csv\"  # Use /tmp for temporary files\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports yesterday's payments data from PostgreSQL to a CSV file.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False  # Indicate failure\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM payments\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '2 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)\n",
    "        if not records:  # Handle empty result set\n",
    "            print(\"No records found for yesterday.\")\n",
    "            return True  # No error, but no data\n",
    "\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description:\n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            csv_writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "        return True  # Indicate success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_file_path):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"cp\", local_file_path, gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully uploaded {local_file_path} to {gcs_file_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during GCS upload: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def load_to_bigquery(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "    \"\"\"Loads data from GCS to BigQuery.\"\"\"\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT column_name, data_type\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'payments';  --  Hardcoded table name.  Good practice to make this a variable.\n",
    "        \"\"\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        schema_list = cursor.fetchall()\n",
    "        if not schema_list:\n",
    "            print(\"Could not retrieve schema from database\")\n",
    "            return False\n",
    "\n",
    "        schema_fields = \",\".join([f\"{row[0]}:{map_postgres_type_to_bq(row[1])}\" for row in schema_list])\n",
    "\n",
    "        command = [\n",
    "            \"bq\", \"load\",\n",
    "            \"--source_format=CSV\",\n",
    "            \"--skip_leading_rows=1\",\n",
    "            \"--autodetect\", #  Added autodetect\n",
    "            f\"{bigquery_dataset_name}.{bigquery_table_name}\",\n",
    "            gcs_file_path,\n",
    "        ]\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully loaded data into BigQuery: {result.stdout}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during BigQuery load: {e.stderr}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def map_postgres_type_to_bq(postgres_type):\n",
    "    \"\"\"Maps PostgreSQL data types to BigQuery data types.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\",\n",
    "        \"bigint\": \"INTEGER\",  # Or possibly \"INT64\"\n",
    "        \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\",\n",
    "        \"decimal\": \"NUMERIC\",\n",
    "        \"real\": \"FLOAT\",\n",
    "        \"double precision\": \"FLOAT\",\n",
    "        \"text\": \"STRING\",\n",
    "        \"character varying\": \"STRING\",\n",
    "        \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\",\n",
    "        \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\",\n",
    "        \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\",\n",
    "        \"time with time zone\": \"TIME\",\n",
    "        \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\",\n",
    "        \"bytea\": \"BYTES\",\n",
    "        \"json\": \"STRING\",  # Or consider using JSON type in BigQuery, requires different load.\n",
    "        \"jsonb\": \"STRING\"\n",
    "    }\n",
    "    return type_mapping.get(postgres_type, \"STRING\")  # Default to STRING if not found\n",
    "\n",
    "def delete_gcs_file(gcs_file_path):\n",
    "    \"\"\"Deletes a file from Google Cloud Storage.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"rm\", gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully deleted {gcs_file_path} from GCS\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deleting GCS file: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Runs the entire data pipeline: export fromPostgres, upload to GCS, load to BigQuery, and delete from GCS.\"\"\"\n",
    "    today = date.today()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "\n",
    "    # Create a table name with the date.\n",
    "    bigquery_table_name = \"yesterday_payments\"\n",
    "\n",
    "    print(f\"Running data pipeline for {yesterday}...\")\n",
    "\n",
    "    if not export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH):\n",
    "        print(\"Pipeline failed: Error exporting data from PostgreSQL.\")\n",
    "        return  # Stop if export fails\n",
    "\n",
    "    gcs_file_path = f\"gs://{GCS_BUCKET_NAME}/raw/{yesterday.strftime('%Y%m%d')}_yesterday_payments.csv\"  # Include date in GCS path\n",
    "    if not upload_to_gcs(OUTPUT_CSV_PATH, GCS_BUCKET_NAME, gcs_file_path):\n",
    "        print(\"Pipeline failed: Error uploading to GCS.\")\n",
    "        return  # Stop if GCS upload fails\n",
    "\n",
    "    if not load_to_bigquery(gcs_file_path, BIGQUERY_DATASET_NAME, bigquery_table_name):\n",
    "        print(\"Pipeline failed: Error loading to BigQuery.\")\n",
    "        return  # Stop if BigQuery load fails\n",
    "\n",
    "    if delete_gcs_file(gcs_file_path):\n",
    "        print(\"Successfully deleted file from GCS.\")\n",
    "    else:\n",
    "        print(\"Warning: Failed to delete file from GCS.  This should be investigated.\")\n",
    "        #  Don't stop the pipeline, but log the error\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data pipeline for all payments updated between 2025-06-25 - 2025-06-26 ...\n",
      "Successfully exported 3051 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\n",
      "Successfully uploaded /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv to gs://ajar-bigquery-staging-bucket/raw/20250625_oltp_payments.csv\n",
      "❌ Error during BigQuery load: \n",
      "Pipeline failed: Error loading to BigQuery.\n"
     ]
    }
   ],
   "source": [
    "# Configuration (Hardcoded with your values - BEST PRACTICE: use environment variables)\n",
    "DB_HOST = \"34.93.7.102\"\n",
    "DB_NAME = \"ajar\"\n",
    "DB_USER = \"tech\"\n",
    "DB_PASSWORD = \">aRSIeB(C,gHuo1|\"\n",
    "GCS_BUCKET_NAME = \"ajar-bigquery-staging-bucket\"\n",
    "BIGQUERY_DATASET_NAME = \"ajar-kw:payments_data\"  # Include the project ID here.\n",
    "BIGQUERY_TABLE_NAME = \"oltp_payments\"  #  No date suffix here, we'll add in the code.\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\"  # Use /tmp for temporary files\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports payments data updated between last two days from PostgreSQL to a CSV file.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False  # Indicate failure\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM payments\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '2 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)\n",
    "        if not records:  # Handle empty result set\n",
    "            print(\"No records found for yesterday.\")\n",
    "            return True  # No error, but no data\n",
    "\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description:\n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            csv_writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "        return True  # Indicate success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_file_path):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"cp\", local_file_path, gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully uploaded {local_file_path} to {gcs_file_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during GCS upload: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "# def load_to_bigquery(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "#     \"\"\"Loads data from GCS to BigQuery.\"\"\"\n",
    "#     try:\n",
    "#         conn = connectionNew()\n",
    "#         if conn is None:\n",
    "#             return False\n",
    "\n",
    "#         query = f\"\"\"\n",
    "#             SELECT column_name, data_type\n",
    "#             FROM information_schema.columns\n",
    "#             WHERE table_name = 'payments';  --  Hardcoded table name.  Good practice to make this a variable.\n",
    "#         \"\"\"\n",
    "#         cursor = conn.cursor()\n",
    "#         cursor.execute(query)\n",
    "#         schema_list = cursor.fetchall()\n",
    "#         if not schema_list:\n",
    "#             print(\"Could not retrieve schema from database\")\n",
    "#             return False\n",
    "\n",
    "#         # Construct the schema string\n",
    "#         schema_fields = \",\".join([\n",
    "#             f\"{row[0]}:{map_postgres_type_to_bq(row[1])}\" for row in schema_list\n",
    "#         ])\n",
    "\n",
    "#         command = [\n",
    "#             \"bq\", \"load\",\n",
    "#             \"--source_format=CSV\",\n",
    "#             \"--skip_leading_rows=1\",\n",
    "#             \"--replace=false\",\n",
    "#             f\"--schema={schema_fields}\",\n",
    "#             f\"{bigquery_dataset_name}.{bigquery_table_name}\",\n",
    "#             gcs_file_path,\n",
    "#         ]\n",
    "\n",
    "#     result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "#     print(f\"Successfully loaded data into BigQuery: {result.stdout}\")\n",
    "#     return True\n",
    "# except subprocess.CalledProcessErcondror as e:\n",
    "#     print(f\"Error during BigQuery load: {e.stderr}\")\n",
    "#     return False\n",
    "# finally:\n",
    "#     if conn:\n",
    "#         conn.close()\n",
    "\n",
    "def load_to_bigquery(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "    \"\"\"Loads data from GCS to BigQuery with explicit schema and appends to existing table.\"\"\"\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False\n",
    "\n",
    "        # Fetch schema from Postgres\n",
    "        query = f\"\"\"\n",
    "            SELECT column_name, data_type\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'payments';\n",
    "        \"\"\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        schema_list = cursor.fetchall()\n",
    "        if not schema_list:\n",
    "            print(\"Could not retrieve schema from database\")\n",
    "            return False\n",
    "\n",
    "        # Map schema to BigQuery-compatible types\n",
    "        schema_fields = \",\".join([\n",
    "            f\"{row[0]}:{map_postgres_type_to_bq(row[1])}\" for row in schema_list\n",
    "        ])\n",
    "\n",
    "        command = [\n",
    "            \"bq\", \"load\",\n",
    "            \"--source_format=CSV\",\n",
    "            \"--skip_leading_rows=1\",\n",
    "            \"--schema_update_option=ALLOW_FIELD_ADDITION\", # Optional: Allows adding new fields if your source schema evolves\n",
    "            # \"--write_disposition=WRITE_APPEND\",  # <--- THIS IS THE KEY CHANGE\n",
    "            f\"--schema={schema_fields}\",  # ✅ use schema instead of autodetect\n",
    "            f\"{bigquery_dataset_name}.{bigquery_table_name}\",\n",
    "            gcs_file_path,\n",
    "        ]\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"✅ Successfully loaded data into BigQuery:\\n{result.stdout}\")\n",
    "        return True\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Error during BigQuery load: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def map_postgres_type_to_bq(postgres_type):\n",
    "    \"\"\"Maps PostgreSQL data types to BigQuery data types.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\",\n",
    "        \"bigint\": \"INTEGER\",  # Or possibly \"INT64\"\n",
    "        \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\",\n",
    "        \"decimal\": \"NUMERIC\",\n",
    "        \"real\": \"FLOAT\",\n",
    "        \"double precision\": \"FLOAT\",\n",
    "        \"text\": \"STRING\",\n",
    "        \"character varying\": \"STRING\",\n",
    "        \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\",\n",
    "        \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\",\n",
    "        \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\",\n",
    "        \"time with time zone\": \"TIME\",\n",
    "        \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\",\n",
    "        \"bytea\": \"BYTES\",\n",
    "        \"json\": \"STRING\",  # Or consider using JSON type in BigQuery, requires different load.\n",
    "        \"jsonb\": \"STRING\"\n",
    "    }\n",
    "    return type_mapping.get(postgres_type, \"STRING\")  # Default to STRING if not found\n",
    "\n",
    "def delete_gcs_file(gcs_file_path):\n",
    "    \"\"\"Deletes a file from Google Cloud Storage.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"rm\", gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully deleted {gcs_file_path} from GCS\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deleting GCS file: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Runs the entire data pipeline: export fromPostgres, upload to GCS, load to BigQuery, and delete from GCS.\"\"\"\n",
    "    today = date.today()\n",
    "    start = today - timedelta(days=2)\n",
    "    end = today - timedelta(days=1)\n",
    "    # Create a table name with the date.\n",
    "    bigquery_table_name = \"oltp_payments\"\n",
    "\n",
    "    print(f\"Running data pipeline for all payments updated between {start} - {end} ...\")\n",
    "\n",
    "    if not export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH):\n",
    "        print(\"Pipeline failed: Error exporting data from PostgreSQL.\")\n",
    "        return  # Stop if export fails\n",
    "\n",
    "    gcs_file_path = f\"gs://{GCS_BUCKET_NAME}/raw/{start.strftime('%Y%m%d')}_oltp_payments.csv\"  # Include date in GCS path\n",
    "    if not upload_to_gcs(OUTPUT_CSV_PATH, GCS_BUCKET_NAME, gcs_file_path):\n",
    "        print(\"Pipeline failed: Error uploading to GCS.\")\n",
    "        return  # Stop if GCS upload fails\n",
    "\n",
    "    if not load_to_bigquery(gcs_file_path, BIGQUERY_DATASET_NAME, bigquery_table_name):\n",
    "        print(\"Pipeline failed: Error loading to BigQuery.\")\n",
    "        return  # Stop if BigQuery load fails\n",
    "\n",
    "    if delete_gcs_file(gcs_file_path):\n",
    "        print(\"Successfully deleted file from GCS.\")\n",
    "    else:\n",
    "        print(\"Warning: Failed to delete file from GCS.  This should be investigated.\")\n",
    "        #  Don't stop the pipeline, but log the error\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data pipeline for payments updated since 2025-06-25 (approximately yesterday and day before)...\n",
      "Successfully exported 3051 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\n",
      "Successfully uploaded /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv to gs://ajar-bigquery-staging-bucket/raw/20250625_oltp_payments.csv\n"
     ]
    },
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 220\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[43mrun_data_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 207\u001b[0m, in \u001b[0;36mrun_data_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# Stop if GCS upload fails\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Call the new BigQuery load function using the client library\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mload_to_bigquery_with_client_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcs_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBIGQUERY_DATASET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBIGQUERY_TABLE_NAME\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline failed: Error loading to BigQuery.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# Stop if BigQuery load fails\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 113\u001b[0m, in \u001b[0;36mload_to_bigquery_with_client_library\u001b[0;34m(gcs_file_path, bigquery_dataset_name, bigquery_table_name)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_to_bigquery_with_client_library\u001b[39m(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads data from GCS to BigQuery using the Python client library.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mbigquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     table_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbigquery_dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbigquery_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# 1. Fetch schema from Postgres (still necessary to define BQ schema for the client library)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ajar/lib/python3.12/site-packages/google/cloud/bigquery/client.py:258\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, project, credentials, _http, location, default_query_job_config, default_load_job_config, client_info, client_options, default_job_creation_mode)\u001b[0m\n\u001b[1;32m    255\u001b[0m     client_options \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mclient_options\u001b[38;5;241m.\u001b[39mfrom_dict(client_options)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# assert isinstance(client_options, google.api_core.client_options.ClientOptions)\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_http\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m kw_args: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: client_info}\n\u001b[1;32m    266\u001b[0m bq_host \u001b[38;5;241m=\u001b[39m _get_bigquery_host()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ajar/lib/python3.12/site-packages/google/cloud/client/__init__.py:338\u001b[0m, in \u001b[0;36mClientWithProject.__init__\u001b[0;34m(self, project, credentials, client_options, _http)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, credentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, client_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _http\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 338\u001b[0m     \u001b[43m_ClientProjectMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     Client\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;28mself\u001b[39m, credentials\u001b[38;5;241m=\u001b[39mcredentials, client_options\u001b[38;5;241m=\u001b[39mclient_options, _http\u001b[38;5;241m=\u001b[39m_http\n\u001b[1;32m    341\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ajar/lib/python3.12/site-packages/google/cloud/client/__init__.py:286\u001b[0m, in \u001b[0;36m_ClientProjectMixin.__init__\u001b[0;34m(self, project, credentials)\u001b[0m\n\u001b[1;32m    283\u001b[0m     project \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(credentials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     project \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject was not passed and could not be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetermined from the environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ajar/lib/python3.12/site-packages/google/cloud/client/__init__.py:305\u001b[0m, in \u001b[0;36m_ClientProjectMixin._determine_default\u001b[0;34m(project)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_determine_default\u001b[39m(project):\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper:  use default project detection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_determine_default_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ajar/lib/python3.12/site-packages/google/cloud/_helpers/__init__.py:152\u001b[0m, in \u001b[0;36m_determine_default_project\u001b[0;34m(project)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determine default project ID explicitly or implicitly as fall-back.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mSee :func:`google.auth.default` for details on how the default project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m:returns: Default project if it can be determined.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     _, project \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m project\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ajar/lib/python3.12/site-packages/google/auth/_default.py:691\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    683\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    687\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[1;32m    688\u001b[0m             )\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[0;32m--> 691\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "# Configuration (Hardcoded - BEST PRACTICE: use environment variables)\n",
    "# For local testing, keep these. For deployment, switch to env vars or Secret Manager.\n",
    "DB_HOST = \"34.93.7.102\"\n",
    "DB_NAME = \"ajar\"\n",
    "DB_USER = \"tech\"\n",
    "DB_PASSWORD = \">aRSIeB(C,gHuo1|\"\n",
    "GCS_BUCKET_NAME = \"ajar-bigquery-staging-bucket\"\n",
    "BIGQUERY_DATASET_NAME = \"ajar-kw:payments_data\"  # Include the project ID here.\n",
    "BIGQUERY_TABLE_NAME = \"oltp_payments\"  # No date suffix here, for appending.\n",
    "# For local execution:\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\"\n",
    "# For cloud environments (e.g., Cloud Functions, Cloud Run, Compute Engine), use /tmp:\n",
    "# OUTPUT_CSV_PATH = \"/tmp/oltp_payments.csv\"\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports payments data updated between last two days from PostgreSQL to a CSV file.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False  # Indicate failure\n",
    "\n",
    "        # Note: (updated_at + interval '3 hours') accounts for a specific timezone offset.\n",
    "        # Ensure this correctly aligns with your data's timezone and your definition of \"yesterday\".\n",
    "        # Current date is June 27, 2025. This query will get data from June 25, 2025 00:00:00 to June 26, 2025 00:00:00 (in the adjusted timezone).\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM payments\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '2 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)\n",
    "        if not records:  # Handle empty result set\n",
    "            print(\"No records found for yesterday.\")\n",
    "            return True  # No error, but no data\n",
    "\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description:\n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            csv_writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "        return True  # Indicate success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_file_path):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"cp\", local_file_path, gcs_file_path]\n",
    "        # capture_output=True, text=True are good for debugging if needed, but not strictly required for cp\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully uploaded {local_file_path} to {gcs_file_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during GCS upload: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def map_postgres_type_to_bq(postgres_type):\n",
    "    \"\"\"Maps PostgreSQL data types to BigQuery data types.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\",\n",
    "        \"bigint\": \"INTEGER\",  # Or \"INT64\"\n",
    "        \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\",\n",
    "        \"decimal\": \"NUMERIC\",\n",
    "        \"real\": \"FLOAT\",\n",
    "        \"double precision\": \"FLOAT\",\n",
    "        \"text\": \"STRING\",\n",
    "        \"character varying\": \"STRING\",\n",
    "        \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\",\n",
    "        \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\",\n",
    "        \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\",\n",
    "        \"time with time zone\": \"TIME\",\n",
    "        \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\",\n",
    "        \"bytea\": \"BYTES\",\n",
    "        \"json\": \"STRING\",\n",
    "        \"jsonb\": \"STRING\"\n",
    "    }\n",
    "    return type_mapping.get(postgres_type, \"STRING\")  # Default to STRING if not found\n",
    "\n",
    "def load_to_bigquery_with_client_library(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "    \"\"\"Loads data from GCS to BigQuery using the Python client library.\"\"\"\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{bigquery_dataset_name}.{bigquery_table_name}\"\n",
    "\n",
    "    # 1. Fetch schema from Postgres (still necessary to define BQ schema for the client library)\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            print(\"Failed to connect to PostgreSQL for schema retrieval.\")\n",
    "            return False\n",
    "\n",
    "        query = \"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'payments';\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        schema_list_pg = cursor.fetchall()\n",
    "        if not schema_list_pg:\n",
    "            print(\"Could not retrieve schema from database. BigQuery load will fail.\")\n",
    "            return False\n",
    "\n",
    "        # Convert PostgreSQL schema to BigQuery schema fields\n",
    "        schema_bq_fields = []\n",
    "        for col_name, pg_type in schema_list_pg:\n",
    "            bq_type = map_postgres_type_to_bq(pg_type)\n",
    "            schema_bq_fields.append(bigquery.SchemaField(col_name, bq_type))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving schema from PostgreSQL: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "    # 2. Configure the BigQuery load job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND, # This is the key for appending\n",
    "        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
    "        schema=schema_bq_fields, # Provide the explicitly derived schema\n",
    "    )\n",
    "\n",
    "    # 3. Start the load job\n",
    "    print(f\"Starting BigQuery load job from {gcs_file_path} to {table_id} using client library...\")\n",
    "    try:\n",
    "        load_job = client.load_table_from_uri(\n",
    "            gcs_file_path, table_id, job_config=job_config\n",
    "        )\n",
    "        load_job.result()  # Waits for the job to complete\n",
    "\n",
    "        print(f\"✅ Successfully loaded {load_job.output_rows} rows into BigQuery table: {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during BigQuery load job (client library): {e}\")\n",
    "        # You can inspect load_job.error_result and load_job.errors for more details\n",
    "        return False\n",
    "\n",
    "def delete_gcs_file(gcs_file_path):\n",
    "    \"\"\"Deletes a file from Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"rm\", gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully deleted {gcs_file_path} from GCS\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deleting GCS file: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Runs the entire data pipeline: export from Postgres, upload to GCS, load to BigQuery, and delete from GCS.\"\"\"\n",
    "    # Current date in Kuwait is Friday, June 27, 2025.\n",
    "    # date_trunc('day', current_date - interval '2 days') will be start of 2025-06-25 (Wednesday)\n",
    "    # date_trunc('day', current_date) will be start of 2025-06-27 (Friday)\n",
    "    # So the query targets data updated from 2025-06-25 00:00:00 up to (but not including) 2025-06-27 00:00:00.\n",
    "    # This captures data for both June 25th and June 26th.\n",
    "    # If you only want June 26th, the query needs adjustment:\n",
    "    # WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '1 day')\n",
    "    # AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "\n",
    "    today = date.today() # 2025-06-27\n",
    "    start_date_for_file_name = today - timedelta(days=2) # This will be 2025-06-25, aligning with the query's start bound.\n",
    "\n",
    "    print(f\"Running data pipeline for payments updated since {start_date_for_file_name} (approximately yesterday and day before)...\")\n",
    "\n",
    "    if not export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH):\n",
    "        print(\"Pipeline failed: Error exporting data from PostgreSQL.\")\n",
    "        return  # Stop if export fails\n",
    "\n",
    "    # Construct the GCS file path using the start_date_for_file_name\n",
    "    gcs_file_path = f\"gs://{GCS_BUCKET_NAME}/raw/{start_date_for_file_name.strftime('%Y%m%d')}_oltp_payments.csv\"\n",
    "    if not upload_to_gcs(OUTPUT_CSV_PATH, GCS_BUCKET_NAME, gcs_file_path):\n",
    "        print(\"Pipeline failed: Error uploading to GCS.\")\n",
    "        return  # Stop if GCS upload fails\n",
    "\n",
    "    # Call the new BigQuery load function using the client library\n",
    "    if not load_to_bigquery_with_client_library(gcs_file_path, BIGQUERY_DATASET_NAME, BIGQUERY_TABLE_NAME):\n",
    "        print(\"Pipeline failed: Error loading to BigQuery.\")\n",
    "        return  # Stop if BigQuery load fails\n",
    "\n",
    "    if delete_gcs_file(gcs_file_path):\n",
    "        print(\"Successfully deleted file from GCS.\")\n",
    "    else:\n",
    "        print(\"Warning: Failed to delete file from GCS. This should be investigated.\")\n",
    "        # Don't stop the pipeline, but log the error\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data pipeline for payments updated since 2025-06-25 (approximately yesterday and day before)...\n",
      "Successfully exported 3051 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\n",
      "Successfully uploaded /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv to gs://ajar-bigquery-staging-bucket/raw/20250625_oltp_payments.csv\n",
      "Starting BigQuery load job from gs://ajar-bigquery-staging-bucket/raw/20250625_oltp_payments.csv to payments_data.oltp_payments using client library...\n",
      "❌ Error during BigQuery load job (client library): 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/ajar-kw/jobs?prettyPrint=false: Field id already exists in schema\n",
      "Pipeline failed: Error loading to BigQuery.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import subprocess\n",
    "from datetime import date, timedelta\n",
    "from google.cloud import bigquery\n",
    "# from google.cloud import storage\n",
    "from google.oauth2 import service_account # <--- ADD THIS IMPORT\n",
    "\n",
    "# Configuration (Hardcoded - BEST PRACTICE: use environment variables)\n",
    "DB_HOST = \"34.93.7.102\"\n",
    "DB_NAME = \"ajar\"\n",
    "DB_USER = \"tech\"\n",
    "DB_PASSWORD = \">aRSIeB(C,gHuo1|\"\n",
    "GCS_BUCKET_NAME = \"ajar-bigquery-staging-bucket\"\n",
    "BIGQUERY_DATASET_NAME = \"payments_data\"  # Include the project ID here.\n",
    "BIGQUERY_TABLE_NAME = \"oltp_payments\"  # No date suffix here, for appending.\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\"\n",
    "\n",
    "# --- ADD THIS CONFIGURATION FOR BIGQUERY SERVICE ACCOUNT ---\n",
    "BIGQUERY_SERVICE_ACCOUNT_KEY_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/pg-bigquery-pipeline.json\"\n",
    "# Ensure the project ID matches what's in your key file, or directly set it below.\n",
    "# If BIGQUERY_DATASET_NAME already has the project ID (e.g., \"ajar-kw:payments_data\"),\n",
    "# you might not strictly need to pass 'project' to Client(), but it's safer to be explicit.\n",
    "BIGQUERY_PROJECT_ID = \"ajar-kw\" # Derived from BIGQUERY_DATASET_NAME, ensure it's accurate\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports payments data updated between last two days from PostgreSQL to a CSV file.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False  # Indicate failure\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM payments\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '2 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)\n",
    "        if not records:\n",
    "            print(\"No records found for yesterday.\")\n",
    "            return True\n",
    "\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description:\n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            csv_writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_file_path):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"cp\", local_file_path, gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully uploaded {local_file_path} to {gcs_file_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during GCS upload: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def map_postgres_type_to_bq(postgres_type):\n",
    "    \"\"\"Maps PostgreSQL data types to BigQuery data types.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\", \"bigint\": \"INTEGER\", \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\", \"decimal\": \"NUMERIC\", \"real\": \"FLOAT\", \"double precision\": \"FLOAT\",\n",
    "        \"text\": \"STRING\", \"character varying\": \"STRING\", \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\", \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\", \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\", \"time with time zone\": \"TIME\", \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\", \"bytea\": \"BYTES\", \"json\": \"STRING\", \"jsonb\": \"STRING\"\n",
    "    }\n",
    "    return type_mapping.get(postgres_type, \"STRING\")\n",
    "\n",
    "def load_to_bigquery_with_client_library(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "    \"\"\"Loads data from GCS to BigQuery using the Python client library.\"\"\"\n",
    "    # --- AUTHENTICATION PART ---\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            BIGQUERY_SERVICE_ACCOUNT_KEY_PATH\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=BIGQUERY_PROJECT_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing BigQuery client with credentials: {e}\")\n",
    "        return False\n",
    "    # --- END AUTHENTICATION PART ---\n",
    "\n",
    "    table_id = f\"{bigquery_dataset_name}.{bigquery_table_name}\"\n",
    "\n",
    "    # 1. Fetch schema from Postgres\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            print(\"Failed to connect to PostgreSQL for schema retrieval.\")\n",
    "            return False\n",
    "\n",
    "        query = \"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'payments';\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        schema_list_pg = cursor.fetchall()\n",
    "        if not schema_list_pg:\n",
    "            print(\"Could not retrieve schema from database. BigQuery load will fail.\")\n",
    "            return False\n",
    "\n",
    "        schema_bq_fields = []\n",
    "        for col_name, pg_type in schema_list_pg:\n",
    "            bq_type = map_postgres_type_to_bq(pg_type)\n",
    "            schema_bq_fields.append(bigquery.SchemaField(col_name, bq_type))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving schema from PostgreSQL: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "    # 2. Configure the BigQuery load job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
    "        schema=schema_bq_fields,\n",
    "    )\n",
    "\n",
    "    # 3. Start the load job\n",
    "    print(f\"Starting BigQuery load job from {gcs_file_path} to {table_id} using client library...\")\n",
    "    try:\n",
    "        load_job = client.load_table_from_uri(\n",
    "            gcs_file_path, table_id, job_config=job_config\n",
    "        )\n",
    "        load_job.result()\n",
    "\n",
    "        print(f\"✅ Successfully loaded {load_job.output_rows} rows into BigQuery table: {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during BigQuery load job (client library): {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_gcs_file(gcs_file_path):\n",
    "    \"\"\"Deletes a file from Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"rm\", gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully deleted {gcs_file_path} from GCS\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deleting GCS file: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Runs the entire data pipeline: export from Postgres, upload to GCS, load to BigQuery, and delete from GCS.\"\"\"\n",
    "    today = date.today()\n",
    "    start_date_for_file_name = today - timedelta(days=2)\n",
    "\n",
    "    print(f\"Running data pipeline for payments updated since {start_date_for_file_name} (approximately yesterday and day before)...\")\n",
    "\n",
    "    if not export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH):\n",
    "        print(\"Pipeline failed: Error exporting data from PostgreSQL.\")\n",
    "        return\n",
    "\n",
    "    gcs_file_path = f\"gs://{GCS_BUCKET_NAME}/raw/{start_date_for_file_name.strftime('%Y%m%d')}_oltp_payments.csv\"\n",
    "    if not upload_to_gcs(OUTPUT_CSV_PATH, GCS_BUCKET_NAME, gcs_file_path):\n",
    "        print(\"Pipeline failed: Error uploading to GCS.\")\n",
    "        return\n",
    "\n",
    "    # Call the new BigQuery load function using the client library\n",
    "    if not load_to_bigquery_with_client_library(gcs_file_path, BIGQUERY_DATASET_NAME, BIGQUERY_TABLE_NAME):\n",
    "        print(\"Pipeline failed: Error loading to BigQuery.\")\n",
    "        return\n",
    "\n",
    "    if delete_gcs_file(gcs_file_path):\n",
    "        print(\"Successfully deleted file from GCS.\")\n",
    "    else:\n",
    "        print(\"Warning: Failed to delete file from GCS. This should be investigated.\")\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data pipeline for payments updated since 2025-06-25 (approximately yesterday and day before)...\n",
      "Successfully exported 3051 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\n",
      "Successfully uploaded /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv to gs://ajar-bigquery-staging-bucket/raw/20250625_oltp_payments.csv\n",
      "Schema being sent to BigQuery:\n",
      "  - id: INTEGER (mode=REQUIRED)\n",
      "  - reference_id: STRING (mode=NULLABLE)\n",
      "  - internal_id: STRING (mode=NULLABLE)\n",
      "  - broker_id: INTEGER (mode=NULLABLE)\n",
      "  - contact_id: INTEGER (mode=NULLABLE)\n",
      "  - user_id: INTEGER (mode=NULLABLE)\n",
      "  - payment_method_id: INTEGER (mode=NULLABLE)\n",
      "  - currency: STRING (mode=NULLABLE)\n",
      "  - offline: BOOLEAN (mode=NULLABLE)\n",
      "  - extra: STRING (mode=NULLABLE)\n",
      "  - amount: FLOAT (mode=NULLABLE)\n",
      "  - refunded_amount: FLOAT (mode=NULLABLE)\n",
      "  - status: STRING (mode=NULLABLE)\n",
      "  - card_bin_id: INTEGER (mode=NULLABLE)\n",
      "  - ip_address: STRING (mode=NULLABLE)\n",
      "  - by_payment_link: BOOLEAN (mode=NULLABLE)\n",
      "  - captured_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - created_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - updated_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - archived_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - created_by: INTEGER (mode=NULLABLE)\n",
      "  - updated_by: INTEGER (mode=NULLABLE)\n",
      "Starting BigQuery load job from gs://ajar-bigquery-staging-bucket/raw/20250625_oltp_payments.csv to payments_data.oltp_payments using client library...\n",
      "❌ Error during BigQuery load job (client library): 400 Provided Schema does not match Table ajar-kw:payments_data.oltp_payments. Field amount has changed type from NUMERIC to FLOAT; reason: invalid, message: Provided Schema does not match Table ajar-kw:payments_data.oltp_payments. Field amount has changed type from NUMERIC to FLOAT\n",
      "Pipeline failed: Error loading to BigQuery.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import subprocess\n",
    "from datetime import date, timedelta\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Configuration (Hardcoded - BEST PRACTICE: use environment variables)\n",
    "DB_HOST = \"34.93.7.102\"\n",
    "DB_NAME = \"ajar\"\n",
    "DB_USER = \"tech\"\n",
    "DB_PASSWORD = \">aRSIeB(C,gHuo1|\"\n",
    "GCS_BUCKET_NAME = \"ajar-bigquery-staging-bucket\"\n",
    "\n",
    "BIGQUERY_PROJECT_ID = \"ajar-kw\" # Explicitly define your project ID\n",
    "BIGQUERY_DATASET_NAME = \"payments_data\" # This should ONLY be the dataset ID\n",
    "\n",
    "BIGQUERY_TABLE_NAME = \"oltp_payments\"\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\"\n",
    "\n",
    "BIGQUERY_SERVICE_ACCOUNT_KEY_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/pg-bigquery-pipeline.json\"\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports payments data updated between last two days from PostgreSQL to a CSV file.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM payments\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '2 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)\n",
    "        if not records:\n",
    "            print(\"No records found for yesterday.\")\n",
    "            return True\n",
    "\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description:\n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            csv_writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_file_path):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"cp\", local_file_path, gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully uploaded {local_file_path} to {gcs_file_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during GCS upload: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def map_postgres_type_to_bq(postgres_type):\n",
    "    \"\"\"Maps PostgreSQL data types to BigQuery data types.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\", \"bigint\": \"INTEGER\", \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\", \"decimal\": \"NUMERIC\", \"real\": \"FLOAT\", \"double precision\": \"FLOAT\",\n",
    "        \"text\": \"STRING\", \"character varying\": \"STRING\", \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\", \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\", \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\", \"time with time zone\": \"TIME\", \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\", \"bytea\": \"BYTES\", \"json\": \"STRING\", \"jsonb\": \"STRING\"\n",
    "    }\n",
    "    return type_mapping.get(postgres_type, \"STRING\")\n",
    "\n",
    "def load_to_bigquery_with_client_library(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "    \"\"\"Loads data from GCS to BigQuery using the Python client library.\"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            BIGQUERY_SERVICE_ACCOUNT_KEY_PATH\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=BIGQUERY_PROJECT_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing BigQuery client with credentials: {e}\")\n",
    "        return False\n",
    "\n",
    "    table_id = f\"{bigquery_dataset_name}.{bigquery_table_name}\"\n",
    "\n",
    "    # 1. Fetch schema from Postgres, including nullability\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            print(\"Failed to connect to PostgreSQL for schema retrieval.\")\n",
    "            return False\n",
    "\n",
    "        # --- MODIFIED QUERY TO GET is_nullable ---\n",
    "        query = f\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'payments'\n",
    "            AND table_schema = 'public'; -- Assuming your table is in the 'public' schema\n",
    "        \"\"\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        schema_list_pg = cursor.fetchall() # Each row is now (col_name, data_type, is_nullable_string)\n",
    "        if not schema_list_pg:\n",
    "            print(\"Could not retrieve schema from database. BigQuery load will fail.\")\n",
    "            return False\n",
    "\n",
    "        schema_bq_fields = []\n",
    "        for col_name, pg_type, is_nullable_str in schema_list_pg:\n",
    "            bq_type = map_postgres_type_to_bq(pg_type)\n",
    "            # --- Dynamically set mode based on PG's is_nullable ---\n",
    "            bq_mode = 'NULLABLE' if is_nullable_str == 'YES' else 'REQUIRED'\n",
    "            schema_bq_fields.append(bigquery.SchemaField(col_name, bq_type, mode=bq_mode))\n",
    "\n",
    "        # --- OPTIONAL: Print schema being sent to BigQuery for debugging ---\n",
    "        print(\"Schema being sent to BigQuery:\")\n",
    "        for field in schema_bq_fields:\n",
    "            print(f\"  - {field.name}: {field.field_type} (mode={field.mode})\")\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving schema from PostgreSQL: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "    # 2. Configure the BigQuery load job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        # Keep ALLOW_FIELD_ADDITION for new columns.\n",
    "        # ALLOW_FIELD_RELAXATION would be needed if you ever change a REQUIRED to NULLABLE in BQ,\n",
    "        # but that's not typically the conflict causing \"id already exists\".\n",
    "        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
    "        schema=schema_bq_fields,\n",
    "    )\n",
    "\n",
    "    # 3. Start the load job\n",
    "    print(f\"Starting BigQuery load job from {gcs_file_path} to {table_id} using client library...\")\n",
    "    try:\n",
    "        load_job = client.load_table_from_uri(\n",
    "            gcs_file_path, table_id, job_config=job_config\n",
    "        )\n",
    "        load_job.result()\n",
    "\n",
    "        print(f\"✅ Successfully loaded {load_job.output_rows} rows into BigQuery table: {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during BigQuery load job (client library): {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_gcs_file(gcs_file_path):\n",
    "    \"\"\"Deletes a file from Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"rm\", gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully deleted {gcs_file_path} from GCS\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deleting GCS file: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Runs the entire data pipeline: export from Postgres, upload to GCS, load to BigQuery, and delete from GCS.\"\"\"\n",
    "    today = date.today()\n",
    "    start_date_for_file_name = today - timedelta(days=2)\n",
    "\n",
    "    print(f\"Running data pipeline for payments updated since {start_date_for_file_name} (approximately yesterday and day before)...\")\n",
    "\n",
    "    if not export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH):\n",
    "        print(\"Pipeline failed: Error exporting data from PostgreSQL.\")\n",
    "        return\n",
    "\n",
    "    gcs_file_path = f\"gs://{GCS_BUCKET_NAME}/raw/{start_date_for_file_name.strftime('%Y%m%d')}_oltp_payments.csv\"\n",
    "    if not upload_to_gcs(OUTPUT_CSV_PATH, GCS_BUCKET_NAME, gcs_file_path):\n",
    "        print(\"Pipeline failed: Error uploading to GCS.\")\n",
    "        return\n",
    "\n",
    "    if not load_to_bigquery_with_client_library(gcs_file_path, BIGQUERY_DATASET_NAME, BIGQUERY_TABLE_NAME):\n",
    "        print(\"Pipeline failed: Error loading to BigQuery.\")\n",
    "        return\n",
    "\n",
    "    if delete_gcs_file(gcs_file_path):\n",
    "        print(\"Successfully deleted file from GCS.\")\n",
    "    else:\n",
    "        print(\"Warning: Failed to delete file from GCS. This should be investigated.\")\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data pipeline for payments updated since 2025-07-01 (approximately yesterday and day before)...\n",
      "Successfully exported 9741 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\n",
      "Successfully uploaded /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv to gs://ajar-bigquery-staging-bucket/raw/20250701_oltp_payments.csv\n",
      "Schema being sent to BigQuery:\n",
      "  - id: INTEGER (mode=REQUIRED)\n",
      "  - reference_id: STRING (mode=NULLABLE)\n",
      "  - internal_id: STRING (mode=NULLABLE)\n",
      "  - broker_id: INTEGER (mode=NULLABLE)\n",
      "  - contact_id: INTEGER (mode=NULLABLE)\n",
      "  - user_id: INTEGER (mode=NULLABLE)\n",
      "  - payment_method_id: INTEGER (mode=NULLABLE)\n",
      "  - currency: STRING (mode=NULLABLE)\n",
      "  - offline: BOOLEAN (mode=NULLABLE)\n",
      "  - extra: STRING (mode=NULLABLE)\n",
      "  - amount: NUMERIC (mode=NULLABLE)\n",
      "  - refunded_amount: NUMERIC (mode=NULLABLE)\n",
      "  - status: STRING (mode=NULLABLE)\n",
      "  - card_bin_id: INTEGER (mode=NULLABLE)\n",
      "  - ip_address: STRING (mode=NULLABLE)\n",
      "  - by_payment_link: BOOLEAN (mode=NULLABLE)\n",
      "  - captured_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - created_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - updated_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - archived_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - created_by: INTEGER (mode=NULLABLE)\n",
      "  - updated_by: INTEGER (mode=NULLABLE)\n",
      "Starting BigQuery load job from gs://ajar-bigquery-staging-bucket/raw/20250701_oltp_payments.csv to payments_data.oltp_payments using client library...\n",
      "✅ Successfully loaded 9741 rows into BigQuery table: payments_data.oltp_payments\n",
      "Successfully deleted gs://ajar-bigquery-staging-bucket/raw/20250701_oltp_payments.csv from GCS\n",
      "Successfully deleted file from GCS.\n",
      "Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import subprocess\n",
    "from datetime import date, timedelta\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Configuration (Hardcoded - BEST PRACTICE: use environment variables)\n",
    "DB_HOST = \"34.93.7.102\"\n",
    "DB_NAME = \"ajar\"\n",
    "DB_USER = \"tech\"\n",
    "DB_PASSWORD = \">aRSIeB(C,gHuo1|\"\n",
    "GCS_BUCKET_NAME = \"ajar-bigquery-staging-bucket\"\n",
    "\n",
    "BIGQUERY_PROJECT_ID = \"ajar-kw\"\n",
    "BIGQUERY_DATASET_NAME = \"payments_data\"\n",
    "BIGQUERY_TABLE_NAME = \"oltp_payments\"\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/oltp_payments.csv\"\n",
    "\n",
    "BIGQUERY_SERVICE_ACCOUNT_KEY_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/pg-bigquery-pipeline.json\"\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports payments data updated between last two days from PostgreSQL to a CSV file.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM payments\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '7 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)\n",
    "        if not records:\n",
    "            print(\"No records found for yesterday.\")\n",
    "            return True\n",
    "\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description:\n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            csv_writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_file_path):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"cp\", local_file_path, gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully uploaded {local_file_path} to {gcs_file_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during GCS upload: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "# --- MODIFIED: Added column_name parameter for specific mapping overrides ---\n",
    "def map_postgres_type_to_bq(postgres_type, column_name=None):\n",
    "    \"\"\"Maps PostgreSQL data types to BigQuery data types, with specific overrides.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\", \"bigint\": \"INTEGER\", \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\", \"decimal\": \"NUMERIC\",\n",
    "        \"real\": \"FLOAT\", \"double precision\": \"FLOAT\", # Default float mapping for these PG types\n",
    "\n",
    "        \"text\": \"STRING\", \"character varying\": \"STRING\", \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\", \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\", \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\", \"time with time zone\": \"TIME\", \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\", \"bytea\": \"BYTES\", \"json\": \"STRING\", \"jsonb\": \"STRING\"\n",
    "    }\n",
    "\n",
    "    # --- Specific override for 'amount' column to ensure it's NUMERIC in BQ ---\n",
    "    if column_name == 'amount' or column_name == 'refunded_amount':\n",
    "        # If your BigQuery 'amount' column is already NUMERIC,\n",
    "        # we must force the incoming schema to also declare it as NUMERIC.\n",
    "        return \"NUMERIC\"\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    return type_mapping.get(postgres_type, \"STRING\") # Default to STRING if not found\n",
    "\n",
    "def load_to_bigquery_with_client_library(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "    \"\"\"Loads data from GCS to BigQuery using the Python client library.\"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            BIGQUERY_SERVICE_ACCOUNT_KEY_PATH\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=BIGQUERY_PROJECT_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing BigQuery client with credentials: {e}\")\n",
    "        return False\n",
    "\n",
    "    table_id = f\"{bigquery_dataset_name}.{bigquery_table_name}\"\n",
    "\n",
    "    # 1. Fetch schema from Postgres, including nullability\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            print(\"Failed to connect to PostgreSQL for schema retrieval.\")\n",
    "            return False\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'payments'\n",
    "            AND table_schema = 'public';\n",
    "        \"\"\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        schema_list_pg = cursor.fetchall()\n",
    "        if not schema_list_pg:\n",
    "            print(\"Could not retrieve schema from database. BigQuery load will fail.\")\n",
    "            return False\n",
    "\n",
    "        schema_bq_fields = []\n",
    "        for col_name, pg_type, is_nullable_str in schema_list_pg:\n",
    "            # --- MODIFIED: Pass column_name to map_postgres_type_to_bq ---\n",
    "            bq_type = map_postgres_type_to_bq(pg_type, column_name=col_name)\n",
    "            # -------------------------------------------------------------\n",
    "            bq_mode = 'NULLABLE' if is_nullable_str == 'YES' else 'REQUIRED'\n",
    "            schema_bq_fields.append(bigquery.SchemaField(col_name, bq_type, mode=bq_mode))\n",
    "\n",
    "        print(\"Schema being sent to BigQuery:\")\n",
    "        for field in schema_bq_fields:\n",
    "            print(f\"  - {field.name}: {field.field_type} (mode={field.mode})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving schema from PostgreSQL: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "    # 2. Configure the BigQuery load job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
    "        schema=schema_bq_fields,\n",
    "    )\n",
    "\n",
    "    # 3. Start the load job\n",
    "    print(f\"Starting BigQuery load job from {gcs_file_path} to {table_id} using client library...\")\n",
    "    try:\n",
    "        load_job = client.load_table_from_uri(\n",
    "            gcs_file_path, table_id, job_config=job_config\n",
    "        )\n",
    "        load_job.result()\n",
    "\n",
    "        print(f\"✅ Successfully loaded {load_job.output_rows} rows into BigQuery table: {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during BigQuery load job (client library): {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_gcs_file(gcs_file_path):\n",
    "    \"\"\"Deletes a file from Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"rm\", gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully deleted {gcs_file_path} from GCS\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deleting GCS file: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Runs the entire data pipeline: export from Postgres, upload to GCS, load to BigQuery, and delete from GCS.\"\"\"\n",
    "    today = date.today()\n",
    "    start_date_for_file_name = today - timedelta(days=2)\n",
    "\n",
    "    print(f\"Running data pipeline for payments updated since {start_date_for_file_name} (approximately yesterday and day before)...\")\n",
    "\n",
    "    if not export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH):\n",
    "        print(\"Pipeline failed: Error exporting data from PostgreSQL.\")\n",
    "        return\n",
    "\n",
    "    gcs_file_path = f\"gs://{GCS_BUCKET_NAME}/raw/{start_date_for_file_name.strftime('%Y%m%d')}_oltp_payments.csv\"\n",
    "    if not upload_to_gcs(OUTPUT_CSV_PATH, GCS_BUCKET_NAME, gcs_file_path):\n",
    "        print(\"Pipeline failed: Error uploading to GCS.\")\n",
    "        return\n",
    "\n",
    "    if not load_to_bigquery_with_client_library(gcs_file_path, BIGQUERY_DATASET_NAME, BIGQUERY_TABLE_NAME):\n",
    "        print(\"Pipeline failed: Error loading to BigQuery.\")\n",
    "        return\n",
    "\n",
    "    if delete_gcs_file(gcs_file_path):\n",
    "        print(\"Successfully deleted file from GCS.\")\n",
    "    else:\n",
    "        print(\"Warning: Failed to delete file from GCS. This should be investigated.\")\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data pipeline for accounts updated since 2025-05-04 (teh time specified)...\n",
      "Successfully exported 124 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/customer_accounts.csv\n",
      "Successfully uploaded /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/customer_accounts.csv to gs://ajar-bigquery-staging-bucket/raw/20250504_customer_accounts.csv\n",
      "Schema being sent to BigQuery:\n",
      "  - id: INTEGER (mode=REQUIRED)\n",
      "  - name: STRING (mode=NULLABLE)\n",
      "  - logo: STRING (mode=NULLABLE)\n",
      "  - kyc_status: STRING (mode=NULLABLE)\n",
      "  - is_registered_company: BOOLEAN (mode=NULLABLE)\n",
      "  - is_on_behalf: BOOLEAN (mode=NULLABLE)\n",
      "  - company_name: STRING (mode=NULLABLE)\n",
      "  - created_by: INTEGER (mode=NULLABLE)\n",
      "  - updated_by: INTEGER (mode=NULLABLE)\n",
      "  - archived_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - created_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - updated_at: TIMESTAMP (mode=NULLABLE)\n",
      "  - contact_email: STRING (mode=NULLABLE)\n",
      "  - contact_phone: STRING (mode=NULLABLE)\n",
      "  - owner_user_id: INTEGER (mode=NULLABLE)\n",
      "Starting BigQuery load job from gs://ajar-bigquery-staging-bucket/raw/20250504_customer_accounts.csv to raw_accounts.customer_accounts using client library...\n",
      "✅ Successfully loaded 124 rows into BigQuery table: raw_accounts.customer_accounts\n",
      "Successfully deleted gs://ajar-bigquery-staging-bucket/raw/20250504_customer_accounts.csv from GCS\n",
      "Successfully deleted file from GCS.\n",
      "Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import subprocess\n",
    "from datetime import date, timedelta\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Configuration (Hardcoded - BEST PRACTICE: use environment variables)\n",
    "DB_HOST = \"34.93.7.102\"\n",
    "DB_NAME = \"ajar\"\n",
    "DB_USER = \"tech\"\n",
    "DB_PASSWORD = \">aRSIeB(C,gHuo1|\"\n",
    "GCS_BUCKET_NAME = \"ajar-bigquery-staging-bucket\"\n",
    "\n",
    "BIGQUERY_PROJECT_ID = \"ajar-kw\"\n",
    "BIGQUERY_DATASET_NAME = \"raw_accounts\"\n",
    "BIGQUERY_TABLE_NAME = \"customer_accounts\"\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/customer_accounts.csv\"\n",
    "\n",
    "BIGQUERY_SERVICE_ACCOUNT_KEY_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/pg-bigquery-pipeline.json\"\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASSWORD)\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports Accounts data updated as specified from PostgreSQL to a CSV file.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return False\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM accounts\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '3 months')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)\n",
    "        if not records:\n",
    "            print(\"No records found for this range.\")\n",
    "            return True\n",
    "\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description:\n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            csv_writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def upload_to_gcs(local_file_path, gcs_bucket_name, gcs_file_path):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"cp\", local_file_path, gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully uploaded {local_file_path} to {gcs_file_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during GCS upload: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "# --- MODIFIED: Added column_name parameter for specific mapping overrides ---\n",
    "def map_postgres_type_to_bq(postgres_type, column_name=None):\n",
    "    \"\"\"Maps PostgreSQL data types to BigQuery data types, with specific overrides.\"\"\"\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\", \"bigint\": \"INTEGER\", \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\", \"decimal\": \"NUMERIC\",\n",
    "        \"real\": \"FLOAT\", \"double precision\": \"FLOAT\", # Default float mapping for these PG types\n",
    "\n",
    "        \"text\": \"STRING\", \"character varying\": \"STRING\", \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\", \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\", \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\", \"time with time zone\": \"TIME\", \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\", \"bytea\": \"BYTES\", \"json\": \"STRING\", \"jsonb\": \"STRING\"\n",
    "    }\n",
    "\n",
    "    # --- Specific override for 'company_name' column to ensure it's NUMERIC in BQ ---\n",
    "    if column_name == 'company_name':\n",
    "    #  or column_name == 'company_name':\n",
    "\n",
    "    #     # If your BigQuery 'amount' column is already NUMERIC,\n",
    "    #     # we must force the incoming schema to also declare it as NUMERIC.\n",
    "        return \"STRING\"\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    return type_mapping.get(postgres_type, \"STRING\") # Default to STRING if not found\n",
    "\n",
    "def load_to_bigquery_with_client_library(gcs_file_path, bigquery_dataset_name, bigquery_table_name):\n",
    "    \"\"\"Loads data from GCS to BigQuery using the Python client library.\"\"\"\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            BIGQUERY_SERVICE_ACCOUNT_KEY_PATH\n",
    "        )\n",
    "        client = bigquery.Client(credentials=credentials, project=BIGQUERY_PROJECT_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing BigQuery client with credentials: {e}\")\n",
    "        return False\n",
    "\n",
    "    table_id = f\"{bigquery_dataset_name}.{bigquery_table_name}\"\n",
    "\n",
    "    # 1. Fetch schema from Postgres, including nullability\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            print(\"Failed to connect to PostgreSQL for schema retrieval.\")\n",
    "            return False\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'accounts'\n",
    "            AND table_schema = 'public';\n",
    "        \"\"\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        schema_list_pg = cursor.fetchall()\n",
    "        if not schema_list_pg:\n",
    "            print(\"Could not retrieve schema from database. BigQuery load will fail.\")\n",
    "            return False\n",
    "\n",
    "        schema_bq_fields = []\n",
    "        for col_name, pg_type, is_nullable_str in schema_list_pg:\n",
    "            # --- MODIFIED: Pass column_name to map_postgres_type_to_bq ---\n",
    "            bq_type = map_postgres_type_to_bq(pg_type, column_name=col_name)\n",
    "            # -------------------------------------------------------------\n",
    "            bq_mode = 'NULLABLE' if is_nullable_str == 'YES' else 'REQUIRED'\n",
    "            schema_bq_fields.append(bigquery.SchemaField(col_name, bq_type, mode=bq_mode))\n",
    "\n",
    "        print(\"Schema being sent to BigQuery:\")\n",
    "        for field in schema_bq_fields:\n",
    "            print(f\"  - {field.name}: {field.field_type} (mode={field.mode})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving schema from PostgreSQL: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "    # 2. Configure the BigQuery load job\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n",
    "        schema=schema_bq_fields,\n",
    "    )\n",
    "\n",
    "    # 3. Start the load job\n",
    "    print(f\"Starting BigQuery load job from {gcs_file_path} to {table_id} using client library...\")\n",
    "    try:\n",
    "        load_job = client.load_table_from_uri(\n",
    "            gcs_file_path, table_id, job_config=job_config\n",
    "        )\n",
    "        load_job.result()\n",
    "\n",
    "        print(f\"✅ Successfully loaded {load_job.output_rows} rows into BigQuery table: {table_id}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during BigQuery load job (client library): {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_gcs_file(gcs_file_path):\n",
    "    \"\"\"Deletes a file from Google Cloud Storage using gsutil.\"\"\"\n",
    "    try:\n",
    "        command = [\"gsutil\", \"rm\", gcs_file_path]\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully deleted {gcs_file_path} from GCS\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error deleting GCS file: {e.stderr}\")\n",
    "        return False\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Runs the entire data pipeline: export from Postgres, upload to GCS, load to BigQuery, and delete from GCS.\"\"\"\n",
    "    today = date.today()\n",
    "    start_date_for_file_name = today - timedelta(days=60)\n",
    "\n",
    "    print(f\"Running data pipeline for accounts updated since {start_date_for_file_name} (teh time specified)...\")\n",
    "\n",
    "    if not export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH):\n",
    "        print(\"Pipeline failed: Error exporting data from PostgreSQL.\")\n",
    "        return\n",
    "\n",
    "    gcs_file_path = f\"gs://{GCS_BUCKET_NAME}/raw/{start_date_for_file_name.strftime('%Y%m%d')}_customer_accounts.csv\"\n",
    "    if not upload_to_gcs(OUTPUT_CSV_PATH, GCS_BUCKET_NAME, gcs_file_path):\n",
    "        print(\"Pipeline failed: Error uploading to GCS.\")\n",
    "        return\n",
    "\n",
    "    if not load_to_bigquery_with_client_library(gcs_file_path, BIGQUERY_DATASET_NAME, BIGQUERY_TABLE_NAME):\n",
    "        print(\"Pipeline failed: Error loading to BigQuery.\")\n",
    "        return\n",
    "\n",
    "    if delete_gcs_file(gcs_file_path):\n",
    "        print(\"Successfully deleted file from GCS.\")\n",
    "    else:\n",
    "        print(\"Warning: Failed to delete file from GCS. This should be investigated.\")\n",
    "\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new platform production URL\n",
    "def connectionNew():\n",
    "    host_name=\"34.93.7.102\"\n",
    "    db_user=\"tech\"\n",
    "    db_password=\">aRSIeB(C,gHuo1|\"\n",
    "    db_name=\"ajar\"\n",
    "    connection = db_connect.connect(host=host_name,user=db_user,password=db_password,database=db_name)\n",
    "    return connection\n",
    "\n",
    "def query_executeNew(query):\n",
    "    \n",
    "    cursor = connectionNew()\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported 582 rows to /Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/yesterday_payments.csv\n"
     ]
    }
   ],
   "source": [
    "# Output CSV file path\n",
    "OUTPUT_CSV_PATH = \"/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/yesterday_payments.csv\"\n",
    "\n",
    "def connectionNew():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database.\"\"\"\n",
    "    host_name = \"34.93.7.102\"\n",
    "    db_user = \"tech\"\n",
    "    db_password = \">aRSIeB(C,gHuo1|\"\n",
    "    db_name = \"ajar\"\n",
    "    try:\n",
    "        connection = db_connect.connect(\n",
    "            host=host_name, user=db_user, password=db_password, database=db_name\n",
    "        )\n",
    "        return connection\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None  # Important: Return None on failure\n",
    "\n",
    "def query_executeNew(connection, query):\n",
    "    \"\"\"Executes a query on the given connection and returns the results.\"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchall(), cursor.description  # Return both data and description\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], None  # Important: Return empty list and None on failure\n",
    "    \n",
    "\n",
    "def export_yesterdays_payments_to_csv(output_csv_path):\n",
    "    \"\"\"Exports yesterday's payments data from PostgreSQL to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        output_csv_path (str): The path to the output CSV file.\n",
    "    \"\"\"\n",
    "    conn = None  # Initialize conn outside the try block\n",
    "    try:\n",
    "        # Connect to PostgreSQL\n",
    "        conn = connectionNew()\n",
    "        if conn is None:\n",
    "            return  # Exit if connection failed\n",
    "\n",
    "        # Query for yesterday's records with 3-hour offset\n",
    "        query = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM payments\n",
    "            WHERE (created_at + interval '3 hours') >= date_trunc('day', current_date - interval '1 day')\n",
    "            AND (created_at + interval '3 hours') < date_trunc('day', current_date);\n",
    "        \"\"\"\n",
    "        records, description = query_executeNew(conn, query)  # Get data and description\n",
    "        if records is None:\n",
    "            return # Exit if the query failed\n",
    "\n",
    "        # Write to CSV\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile: # Added encoding\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            if description: #check if description exists\n",
    "                # Write header \n",
    "                column_names = [desc[0] for desc in description]\n",
    "                csv_writer.writerow(column_names)\n",
    "            # Write data rows\n",
    "            csv_writer.writerows(records)\n",
    "\n",
    "        print(f\"Successfully exported {len(records)} rows to {output_csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PostgreSQL export: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()  # Use conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_yesterdays_payments_to_csv(OUTPUT_CSV_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
