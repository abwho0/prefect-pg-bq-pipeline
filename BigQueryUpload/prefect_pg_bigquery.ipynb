{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:41:31.735 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tricky-lemming'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tricky-lemming'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'nightly-pg-to-bq'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:41:31.735 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'tricky-lemming'\u001b[0m - Beginning flow run\u001b[35m 'tricky-lemming'\u001b[0m for flow\u001b[1;35m 'nightly-pg-to-bq'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: raw_accounts.customer_accounts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:41:35.180 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_pg_column_types-e49' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:41:35.180 | \u001b[36mINFO\u001b[0m    | Task run 'get_pg_column_types-e49' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:41:38.296 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract_data-fa2' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:41:38.296 | \u001b[36mINFO\u001b[0m    | Task run 'extract_data-fa2' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 9 rows from accounts\n",
      "Original DataFrame dtypes:\n",
      "id                                     int64\n",
      "name                                  object\n",
      "logo                                  object\n",
      "kyc_status                            object\n",
      "is_registered_company                 object\n",
      "is_on_behalf                            bool\n",
      "company_name                          object\n",
      "created_by                           float64\n",
      "updated_by                           float64\n",
      "archived_at                           object\n",
      "created_at               datetime64[ns, UTC]\n",
      "updated_at               datetime64[ns, UTC]\n",
      "contact_email                         object\n",
      "contact_phone                         object\n",
      "owner_user_id                          int64\n",
      "dtype: object\n",
      "Found datetime columns: ['created_at', 'updated_at']\n",
      "Processing datetime column: created_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "Processing datetime column: updated_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "DataFrame dtypes after datetime fix:\n",
      "id                                int64\n",
      "name                             object\n",
      "logo                             object\n",
      "kyc_status                       object\n",
      "is_registered_company            object\n",
      "is_on_behalf                       bool\n",
      "company_name                     object\n",
      "created_by                      float64\n",
      "updated_by                      float64\n",
      "archived_at                      object\n",
      "created_at               datetime64[ns]\n",
      "updated_at               datetime64[ns]\n",
      "contact_email                    object\n",
      "contact_phone                    object\n",
      "owner_user_id                     int64\n",
      "dtype: object\n",
      "Loaded 9 rows to ajar-kw.raw_accounts.customer_accounts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:41:45.041 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'load_to_bigquery-b96' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:41:45.041 | \u001b[36mINFO\u001b[0m    | Task run 'load_to_bigquery-b96' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: payments_data.yesterday_payments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:41:48.375 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_pg_column_types-86c' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:41:48.375 | \u001b[36mINFO\u001b[0m    | Task run 'get_pg_column_types-86c' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:41:55.739 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract_data-aa9' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:41:55.739 | \u001b[36mINFO\u001b[0m    | Task run 'extract_data-aa9' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 9169 rows from payments\n",
      "Original DataFrame dtypes:\n",
      "id                                 int64\n",
      "reference_id                      object\n",
      "internal_id                       object\n",
      "broker_id                          int64\n",
      "contact_id                         int64\n",
      "user_id                          float64\n",
      "payment_method_id                float64\n",
      "currency                          object\n",
      "offline                             bool\n",
      "extra                             object\n",
      "amount                           float64\n",
      "refunded_amount                  float64\n",
      "status                            object\n",
      "card_bin_id                       object\n",
      "ip_address                        object\n",
      "by_payment_link                   object\n",
      "captured_at          datetime64[ns, UTC]\n",
      "created_at           datetime64[ns, UTC]\n",
      "updated_at           datetime64[ns, UTC]\n",
      "archived_at                       object\n",
      "created_by                       float64\n",
      "updated_by                       float64\n",
      "dtype: object\n",
      "Found datetime columns: ['captured_at', 'created_at', 'updated_at']\n",
      "Processing datetime column: captured_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "Processing datetime column: created_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "Processing datetime column: updated_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "DataFrame dtypes after datetime fix:\n",
      "id                            int64\n",
      "reference_id                 object\n",
      "internal_id                  object\n",
      "broker_id                     int64\n",
      "contact_id                    int64\n",
      "user_id                     float64\n",
      "payment_method_id           float64\n",
      "currency                     object\n",
      "offline                        bool\n",
      "extra                        object\n",
      "amount                      float64\n",
      "refunded_amount             float64\n",
      "status                       object\n",
      "card_bin_id                  object\n",
      "ip_address                   object\n",
      "by_payment_link              object\n",
      "captured_at          datetime64[ns]\n",
      "created_at           datetime64[ns]\n",
      "updated_at           datetime64[ns]\n",
      "archived_at                  object\n",
      "created_by                  float64\n",
      "updated_by                  float64\n",
      "dtype: object\n",
      "Loaded 9169 rows to ajar-kw.payments_data.yesterday_payments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:42:01.904 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'load_to_bigquery-bf3' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:42:01.904 | \u001b[36mINFO\u001b[0m    | Task run 'load_to_bigquery-bf3' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:42:01.989 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tricky-lemming'</span> - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:42:01.989 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'tricky-lemming'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from prefect import flow, task\n",
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, inspect\n",
    "# from google.cloud import bigquery\n",
    "# from google.oauth2 import service_account\n",
    "# from google.cloud.bigquery import SchemaField\n",
    "# import os\n",
    "# import pytz\n",
    "\n",
    "# # --- Your custom type mapping function ---\n",
    "# def map_postgres_type_to_bq(postgres_type, column_name=None):\n",
    "#     type_mapping = {\n",
    "#         \"integer\": \"INTEGER\", \"bigint\": \"INTEGER\", \"smallint\": \"INTEGER\",\n",
    "#         \"numeric\": \"NUMERIC\", \"decimal\": \"NUMERIC\",\n",
    "#         \"real\": \"FLOAT\", \"double precision\": \"FLOAT\",\n",
    "#         \"text\": \"STRING\", \"character varying\": \"STRING\", \"varchar\": \"STRING\",\n",
    "#         \"char\": \"STRING\", \"character\": \"STRING\",\n",
    "#         \"timestamp with time zone\": \"TIMESTAMP\", \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "#         \"date\": \"DATE\", \"time with time zone\": \"TIME\", \"time without time zone\": \"TIME\",\n",
    "#         \"boolean\": \"BOOLEAN\", \"bytea\": \"BYTES\", \"json\": \"STRING\", \"jsonb\": \"STRING\"\n",
    "#     }\n",
    "\n",
    "#     # Specific overrides\n",
    "#     if column_name in ['amount', 'refunded_amount']:\n",
    "#         return \"NUMERIC\"\n",
    "#     if column_name == 'company_name':\n",
    "#         return \"STRING\"\n",
    "\n",
    "#     return type_mapping.get(postgres_type.lower(), \"STRING\")\n",
    "\n",
    "# # --- Task to get PostgreSQL column types ---\n",
    "# @task\n",
    "# def get_pg_column_types(table_name, conn_str):\n",
    "#     engine = create_engine(conn_str)\n",
    "#     inspector = inspect(engine)\n",
    "#     columns = inspector.get_columns(table_name)\n",
    "#     col_types = {col['name']: str(col['type']) for col in columns}\n",
    "#     return col_types\n",
    "\n",
    "# # --- Task to extract data from PostgreSQL ---\n",
    "# @task\n",
    "# def extract_data(query, conn_str):\n",
    "#     engine = create_engine(conn_str)\n",
    "#     df = pd.read_sql(query, engine)\n",
    "#     return df\n",
    "\n",
    "# # --- Helper to build BigQuery schema ---\n",
    "# def build_bq_schema(df, pg_column_types):\n",
    "#     schema = []\n",
    "#     for col in df.columns:\n",
    "#         pg_type = pg_column_types.get(col, None)\n",
    "#         bq_type = map_postgres_type_to_bq(pg_type, column_name=col)\n",
    "#         schema.append(SchemaField(name=col, field_type=bq_type, mode=\"NULLABLE\"))\n",
    "#     return schema\n",
    "\n",
    "# # --- WORKING function to fix datetime columns ---\n",
    "# def fix_datetime_columns(df):\n",
    "#     \"\"\"\n",
    "#     Aggressively fix datetime columns to work with BigQuery\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "    \n",
    "#     # Find all datetime columns\n",
    "#     datetime_cols = []\n",
    "#     for col in df_copy.columns:\n",
    "#         if 'datetime64' in str(df_copy[col].dtype) or 'timestamp' in str(df_copy[col].dtype).lower():\n",
    "#             datetime_cols.append(col)\n",
    "    \n",
    "#     print(f\"Found datetime columns: {datetime_cols}\")\n",
    "    \n",
    "#     for col in datetime_cols:\n",
    "#         print(f\"Processing datetime column: {col}\")\n",
    "#         print(f\"Original dtype: {df_copy[col].dtype}\")\n",
    "        \n",
    "#         # Convert to string first, then back to datetime (removes all timezone info)\n",
    "#         df_copy[col] = df_copy[col].astype(str)\n",
    "#         df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "        \n",
    "#         # Ensure it's timezone-naive\n",
    "#         if hasattr(df_copy[col].dtype, 'tz') and df_copy[col].dtype.tz is not None:\n",
    "#             df_copy[col] = df_copy[col].dt.tz_localize(None)\n",
    "        \n",
    "#         print(f\"Final dtype: {df_copy[col].dtype}\")\n",
    "    \n",
    "#     return df_copy\n",
    "\n",
    "# # --- Task to load data into BigQuery ---\n",
    "# @task\n",
    "# def load_to_bigquery(df, table_id, bq_schema):\n",
    "#     print(f\"Original DataFrame dtypes:\")\n",
    "#     print(df.dtypes)\n",
    "    \n",
    "#     # Fix datetime columns\n",
    "#     df_fixed = fix_datetime_columns(df)\n",
    "    \n",
    "#     print(f\"DataFrame dtypes after datetime fix:\")\n",
    "#     print(df_fixed.dtypes)\n",
    "    \n",
    "#     # Handle other data types\n",
    "#     for col in df_fixed.columns:\n",
    "#         col_bq_type = None\n",
    "#         for schema_field in bq_schema:\n",
    "#             if schema_field.name == col:\n",
    "#                 col_bq_type = schema_field.field_type\n",
    "#                 break\n",
    "        \n",
    "#         if df_fixed[col].dtype == 'object' and col_bq_type != 'TIMESTAMP':\n",
    "#             if col_bq_type == 'BOOLEAN':\n",
    "#                 df_fixed[col] = df_fixed[col].map({\n",
    "#                     'True': True, 'true': True, 'TRUE': True, True: True,\n",
    "#                     'False': False, 'false': False, 'FALSE': False, False: False,\n",
    "#                     'None': None, None: None, 'null': None, 'NULL': None\n",
    "#                 })\n",
    "#             else:\n",
    "#                 df_fixed[col] = df_fixed[col].astype(str)\n",
    "#                 df_fixed[col] = df_fixed[col].replace('None', None)\n",
    "    \n",
    "#     # Load credentials\n",
    "#     credentials_path = '/Users/abdullahajmal/Abdullah@Ajar/BigQueryUpload/pg-bigquery-pipeline.json'\n",
    "#     credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "#     client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "#     # Use auto-detect schema instead of manual schema\n",
    "#     job_config = bigquery.LoadJobConfig(\n",
    "#         write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "#         autodetect=True,  # Let BigQuery figure out the schema\n",
    "#         ignore_unknown_values=True,\n",
    "#         allow_quoted_newlines=True,\n",
    "#         allow_jagged_rows=True,\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         load_job = client.load_table_from_dataframe(df_fixed, table_id, job_config=job_config)\n",
    "#         load_job.result()\n",
    "#         print(f\"Loaded {load_job.output_rows} rows to {table_id}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading data to BigQuery: {e}\")\n",
    "#         print(f\"DataFrame dtypes: {df_fixed.dtypes}\")\n",
    "#         print(f\"DataFrame shape: {df_fixed.shape}\")\n",
    "        \n",
    "#         # Try one more time with string conversion for datetime columns\n",
    "#         print(\"Attempting fallback: converting datetime columns to strings...\")\n",
    "#         for col in df_fixed.columns:\n",
    "#             if 'datetime64' in str(df_fixed[col].dtype):\n",
    "#                 df_fixed[col] = df_fixed[col].astype(str)\n",
    "#                 print(f\"Converted {col} to string\")\n",
    "        \n",
    "#         try:\n",
    "#             load_job = client.load_table_from_dataframe(df_fixed, table_id, job_config=job_config)\n",
    "#             load_job.result()\n",
    "#             print(f\"Loaded {load_job.output_rows} rows to {table_id} (with string datetime conversion)\")\n",
    "#         except Exception as e2:\n",
    "#             print(f\"Final error: {e2}\")\n",
    "#             raise\n",
    "\n",
    "# # --- Main Prefect flow ---\n",
    "# @flow\n",
    "# def nightly_pg_to_bq():\n",
    "#     pg_conn_str = \"postgresql+psycopg2://tech:>aRSIeB(C,gHuo1|@34.18.1.152/ajar\"\n",
    "#     queries = {\n",
    "#         \"raw_accounts.customer_accounts\": \"\"\"\n",
    "#             SELECT * FROM accounts\n",
    "#             WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '7 days')\n",
    "#             AND (updated_at + interval '3 hours') < date_trunc('day', current_date + interval '1 day');\n",
    "#         \"\"\",\n",
    "#         \"payments_data.yesterday_payments\": \"\"\"\n",
    "#             SELECT * FROM payments\n",
    "#             WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '7 days')\n",
    "#             AND (updated_at + interval '3 hours') < date_trunc('day', current_date + interval '1 day');\n",
    "#         \"\"\"\n",
    "#     }\n",
    "\n",
    "#     bq_to_pg_table_map = {\n",
    "#         \"raw_accounts.customer_accounts\": \"accounts\",\n",
    "#         \"payments_data.yesterday_payments\": \"payments\"\n",
    "#     }\n",
    "\n",
    "#     for bq_table, sql in queries.items():\n",
    "#         print(f\"Processing table: {bq_table}\")\n",
    "        \n",
    "#         pg_table_name = bq_to_pg_table_map[bq_table]\n",
    "#         pg_col_types = get_pg_column_types(pg_table_name, pg_conn_str)\n",
    "#         df = extract_data(sql, pg_conn_str)\n",
    "        \n",
    "#         print(f\"Extracted {len(df)} rows from {pg_table_name}\")\n",
    "        \n",
    "#         bq_schema = build_bq_schema(df, pg_col_types)\n",
    "#         load_to_bigquery(df, f\"ajar-kw.{bq_table}\", bq_schema)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     nightly_pg_to_bq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original path: /Users/abdullahajmal/Desktop/dbt-connection/BigQueryUpload/pg-bigquery-pipeline.json\n",
      "Absolute path: /Users/abdullahajmal/Desktop/dbt-connection/BigQueryUpload/pg-bigquery-pipeline.json\n",
      "Expanded path: /Users/abdullahajmal/Desktop/dbt-connection/BigQueryUpload/pg-bigquery-pipeline.json\n",
      "File exists: True\n",
      "Current working directory: /Users/abdullahajmal/Desktop/dbt-connection-clean/BigQueryUpload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:02:45.647 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'slick-mouse'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'slick-mouse'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'nightly-pg-to-bq'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:02:45.647 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'slick-mouse'\u001b[0m - Beginning flow run\u001b[35m 'slick-mouse'\u001b[0m for flow\u001b[1;35m 'nightly-pg-to-bq'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:02:45.655 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'slick-mouse'</span> - View at <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://app.prefect.cloud/account/f1d68e12-e0ca-48dd-8884-fa8ce83ae7cc/workspace/bac622a6-084e-444a-bff9-36b04e80bc8d/runs/flow-run/0686d250-37d3-7652-8000-be4477457cd3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:02:45.655 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'slick-mouse'\u001b[0m - View at \u001b[94mhttps://app.prefect.cloud/account/f1d68e12-e0ca-48dd-8884-fa8ce83ae7cc/workspace/bac622a6-084e-444a-bff9-36b04e80bc8d/runs/flow-run/0686d250-37d3-7652-8000-be4477457cd3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: raw_accounts.customer_accounts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:02:47.527 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_pg_column_types-878' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:02:47.527 | \u001b[36mINFO\u001b[0m    | Task run 'get_pg_column_types-878' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:02:49.156 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract_data-a60' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:02:49.156 | \u001b[36mINFO\u001b[0m    | Task run 'extract_data-a60' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12 rows from accounts\n",
      "Original DataFrame dtypes:\n",
      "id                                     int64\n",
      "name                                  object\n",
      "logo                                  object\n",
      "kyc_status                            object\n",
      "is_registered_company                 object\n",
      "is_on_behalf                            bool\n",
      "company_name                          object\n",
      "created_by                           float64\n",
      "updated_by                           float64\n",
      "archived_at                           object\n",
      "created_at               datetime64[ns, UTC]\n",
      "updated_at               datetime64[ns, UTC]\n",
      "contact_email                         object\n",
      "contact_phone                         object\n",
      "owner_user_id                          int64\n",
      "dtype: object\n",
      "Found datetime columns: ['created_at', 'updated_at']\n",
      "Processing datetime column: created_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "Processing datetime column: updated_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "DataFrame dtypes after datetime fix:\n",
      "id                                int64\n",
      "name                             object\n",
      "logo                             object\n",
      "kyc_status                       object\n",
      "is_registered_company            object\n",
      "is_on_behalf                       bool\n",
      "company_name                     object\n",
      "created_by                      float64\n",
      "updated_by                      float64\n",
      "archived_at                      object\n",
      "created_at               datetime64[ns]\n",
      "updated_at               datetime64[ns]\n",
      "contact_email                    object\n",
      "contact_phone                    object\n",
      "owner_user_id                     int64\n",
      "dtype: object\n",
      "Credential path being used: /Users/abdullahajmal/Desktop/dbt-connection/BigQueryUpload/pg-bigquery-pipeline.json\n",
      "Loaded 12 rows to ajar-kw.raw_accounts.customer_accounts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:02:57.001 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'load_to_bigquery-048' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:02:57.001 | \u001b[36mINFO\u001b[0m    | Task run 'load_to_bigquery-048' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: payments_data.yesterday_payments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:02:58.671 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'get_pg_column_types-b8c' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:02:58.671 | \u001b[36mINFO\u001b[0m    | Task run 'get_pg_column_types-b8c' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:03:02.259 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract_data-f21' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:03:02.259 | \u001b[36mINFO\u001b[0m    | Task run 'extract_data-f21' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 8214 rows from payments\n",
      "Original DataFrame dtypes:\n",
      "id                                 int64\n",
      "reference_id                      object\n",
      "internal_id                       object\n",
      "broker_id                          int64\n",
      "contact_id                         int64\n",
      "user_id                          float64\n",
      "payment_method_id                float64\n",
      "currency                          object\n",
      "offline                             bool\n",
      "extra                             object\n",
      "amount                           float64\n",
      "refunded_amount                  float64\n",
      "status                            object\n",
      "card_bin_id                       object\n",
      "ip_address                        object\n",
      "by_payment_link                   object\n",
      "captured_at          datetime64[ns, UTC]\n",
      "created_at           datetime64[ns, UTC]\n",
      "updated_at           datetime64[ns, UTC]\n",
      "archived_at                       object\n",
      "created_by                       float64\n",
      "updated_by                       float64\n",
      "dtype: object\n",
      "Found datetime columns: ['captured_at', 'created_at', 'updated_at']\n",
      "Processing datetime column: captured_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "Processing datetime column: created_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "Processing datetime column: updated_at\n",
      "Original dtype: datetime64[ns, UTC]\n",
      "Final dtype: datetime64[ns]\n",
      "DataFrame dtypes after datetime fix:\n",
      "id                            int64\n",
      "reference_id                 object\n",
      "internal_id                  object\n",
      "broker_id                     int64\n",
      "contact_id                    int64\n",
      "user_id                     float64\n",
      "payment_method_id           float64\n",
      "currency                     object\n",
      "offline                        bool\n",
      "extra                        object\n",
      "amount                      float64\n",
      "refunded_amount             float64\n",
      "status                       object\n",
      "card_bin_id                  object\n",
      "ip_address                   object\n",
      "by_payment_link              object\n",
      "captured_at          datetime64[ns]\n",
      "created_at           datetime64[ns]\n",
      "updated_at           datetime64[ns]\n",
      "archived_at                  object\n",
      "created_by                  float64\n",
      "updated_by                  float64\n",
      "dtype: object\n",
      "Credential path being used: /Users/abdullahajmal/Desktop/dbt-connection/BigQueryUpload/pg-bigquery-pipeline.json\n",
      "Loaded 8214 rows to ajar-kw.payments_data.yesterday_payments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:03:08.084 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'load_to_bigquery-65c' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:03:08.084 | \u001b[36mINFO\u001b[0m    | Task run 'load_to_bigquery-65c' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">17:03:08.568 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'slick-mouse'</span> - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "17:03:08.568 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'slick-mouse'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/Users/abdullahajmal/Desktop/dbt-connection-clean.env\")\n",
    "\n",
    "\n",
    "# Now fetch the secrets\n",
    "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "print(f\"Original path: {credentials_path}\")\n",
    "print(f\"Absolute path: {os.path.abspath(credentials_path)}\")\n",
    "print(f\"Expanded path: {os.path.expanduser(credentials_path)}\")\n",
    "print(f\"File exists: {os.path.exists(credentials_path)}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "\n",
    "from prefect import flow, task\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import os\n",
    "import pytz\n",
    "\n",
    "# --- Your custom type mapping function ---\n",
    "def map_postgres_type_to_bq(postgres_type, column_name=None):\n",
    "    type_mapping = {\n",
    "        \"integer\": \"INTEGER\", \"bigint\": \"INTEGER\", \"smallint\": \"INTEGER\",\n",
    "        \"numeric\": \"NUMERIC\", \"decimal\": \"NUMERIC\",\n",
    "        \"real\": \"FLOAT\", \"double precision\": \"FLOAT\",\n",
    "        \"text\": \"STRING\", \"character varying\": \"STRING\", \"varchar\": \"STRING\",\n",
    "        \"char\": \"STRING\", \"character\": \"STRING\",\n",
    "        \"timestamp with time zone\": \"TIMESTAMP\", \"timestamp without time zone\": \"TIMESTAMP\",\n",
    "        \"date\": \"DATE\", \"time with time zone\": \"TIME\", \"time without time zone\": \"TIME\",\n",
    "        \"boolean\": \"BOOLEAN\", \"bytea\": \"BYTES\", \"json\": \"STRING\", \"jsonb\": \"STRING\"\n",
    "    }\n",
    "\n",
    "    # Specific overrides\n",
    "    if column_name in ['amount', 'refunded_amount']:\n",
    "        return \"NUMERIC\"\n",
    "    if column_name == 'company_name':\n",
    "        return \"STRING\"\n",
    "\n",
    "    return type_mapping.get(postgres_type.lower(), \"STRING\")\n",
    "\n",
    "# --- Task to get PostgreSQL column types ---\n",
    "@task\n",
    "def get_pg_column_types(table_name, conn_str):\n",
    "    engine = create_engine(conn_str)\n",
    "    inspector = inspect(engine)\n",
    "    columns = inspector.get_columns(table_name)\n",
    "    col_types = {col['name']: str(col['type']) for col in columns}\n",
    "    return col_types\n",
    "\n",
    "# --- Task to extract data from PostgreSQL ---\n",
    "@task\n",
    "def extract_data(query, conn_str):\n",
    "    engine = create_engine(conn_str)\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df\n",
    "\n",
    "# --- Helper to build BigQuery schema ---\n",
    "def build_bq_schema(df, pg_column_types):\n",
    "    schema = []\n",
    "    for col in df.columns:\n",
    "        pg_type = pg_column_types.get(col, None)\n",
    "        bq_type = map_postgres_type_to_bq(pg_type, column_name=col)\n",
    "        schema.append(SchemaField(name=col, field_type=bq_type, mode=\"NULLABLE\"))\n",
    "    return schema\n",
    "\n",
    "# --- WORKING function to fix datetime columns ---\n",
    "def fix_datetime_columns(df):\n",
    "    \"\"\"\n",
    "    Aggressively fix datetime columns to work with BigQuery\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find all datetime columns\n",
    "    datetime_cols = []\n",
    "    for col in df_copy.columns:\n",
    "        if 'datetime64' in str(df_copy[col].dtype) or 'timestamp' in str(df_copy[col].dtype).lower():\n",
    "            datetime_cols.append(col)\n",
    "    \n",
    "    print(f\"Found datetime columns: {datetime_cols}\")\n",
    "    \n",
    "    for col in datetime_cols:\n",
    "        print(f\"Processing datetime column: {col}\")\n",
    "        print(f\"Original dtype: {df_copy[col].dtype}\")\n",
    "        \n",
    "        # Convert to string first, then back to datetime (removes all timezone info)\n",
    "        df_copy[col] = df_copy[col].astype(str)\n",
    "        df_copy[col] = pd.to_datetime(df_copy[col], errors='coerce')\n",
    "        \n",
    "        # Ensure it's timezone-naive\n",
    "        if hasattr(df_copy[col].dtype, 'tz') and df_copy[col].dtype.tz is not None:\n",
    "            df_copy[col] = df_copy[col].dt.tz_localize(None)\n",
    "        \n",
    "        print(f\"Final dtype: {df_copy[col].dtype}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# --- Task to load data into BigQuery ---\n",
    "@task\n",
    "def load_to_bigquery(df, table_id, bq_schema):\n",
    "    print(f\"Original DataFrame dtypes:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Fix datetime columns\n",
    "    df_fixed = fix_datetime_columns(df)\n",
    "    \n",
    "    print(f\"DataFrame dtypes after datetime fix:\")\n",
    "    print(df_fixed.dtypes)\n",
    "    \n",
    "    # Handle other data types\n",
    "    for col in df_fixed.columns:\n",
    "        col_bq_type = None\n",
    "        for schema_field in bq_schema:\n",
    "            if schema_field.name == col:\n",
    "                col_bq_type = schema_field.field_type\n",
    "                break\n",
    "        \n",
    "        if df_fixed[col].dtype == 'object' and col_bq_type != 'TIMESTAMP':\n",
    "            if col_bq_type == 'BOOLEAN':\n",
    "                df_fixed[col] = df_fixed[col].map({\n",
    "                    'True': True, 'true': True, 'TRUE': True, True: True,\n",
    "                    'False': False, 'false': False, 'FALSE': False, False: False,\n",
    "                    'None': None, None: None, 'null': None, 'NULL': None\n",
    "                })\n",
    "            else:\n",
    "                df_fixed[col] = df_fixed[col].astype(str)\n",
    "                df_fixed[col] = df_fixed[col].replace('None', None)\n",
    "    \n",
    "    # Load credentials\n",
    "    credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "    print(\"Credential path being used:\", credentials_path)\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "    # Use auto-detect schema instead of manual schema\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        autodetect=True,  # Let BigQuery figure out the schema\n",
    "        ignore_unknown_values=True,\n",
    "        allow_quoted_newlines=True,\n",
    "        allow_jagged_rows=True,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        load_job = client.load_table_from_dataframe(df_fixed, table_id, job_config=job_config)\n",
    "        load_job.result()\n",
    "        print(f\"Loaded {load_job.output_rows} rows to {table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to BigQuery: {e}\")\n",
    "        print(f\"DataFrame dtypes: {df_fixed.dtypes}\")\n",
    "        print(f\"DataFrame shape: {df_fixed.shape}\")\n",
    "        \n",
    "        # Try one more time with string conversion for datetime columns\n",
    "        print(\"Attempting fallback: converting datetime columns to strings...\")\n",
    "        for col in df_fixed.columns:\n",
    "            if 'datetime64' in str(df_fixed[col].dtype):\n",
    "                df_fixed[col] = df_fixed[col].astype(str)\n",
    "                print(f\"Converted {col} to string\")\n",
    "        \n",
    "        try:\n",
    "            load_job = client.load_table_from_dataframe(df_fixed, table_id, job_config=job_config)\n",
    "            load_job.result()\n",
    "            print(f\"Loaded {load_job.output_rows} rows to {table_id} (with string datetime conversion)\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Final error: {e2}\")\n",
    "            raise\n",
    "\n",
    "# --- Main Prefect flow ---\n",
    "@flow\n",
    "def nightly_pg_to_bq():\n",
    "    pg_conn_str = f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "    queries = {\n",
    "        \"raw_accounts.customer_accounts\": \"\"\"\n",
    "            SELECT * FROM accounts\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '7 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date + interval '1 day');\n",
    "        \"\"\",\n",
    "        \"payments_data.yesterday_payments\": \"\"\"\n",
    "            SELECT * FROM payments\n",
    "            WHERE (updated_at + interval '3 hours') >= date_trunc('day', current_date - interval '7 days')\n",
    "            AND (updated_at + interval '3 hours') < date_trunc('day', current_date + interval '1 day');\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    bq_to_pg_table_map = {\n",
    "        \"raw_accounts.customer_accounts\": \"accounts\",\n",
    "        \"payments_data.yesterday_payments\": \"payments\"\n",
    "    }\n",
    "\n",
    "    for bq_table, sql in queries.items():\n",
    "        print(f\"Processing table: {bq_table}\")\n",
    "        \n",
    "        pg_table_name = bq_to_pg_table_map[bq_table]\n",
    "        pg_col_types = get_pg_column_types(pg_table_name, pg_conn_str)\n",
    "        df = extract_data(sql, pg_conn_str)\n",
    "        \n",
    "        print(f\"Extracted {len(df)} rows from {pg_table_name}\")\n",
    "        \n",
    "        bq_schema = build_bq_schema(df, pg_col_types)\n",
    "        load_to_bigquery(df, f\"ajar-kw.{bq_table}\", bq_schema)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nightly_pg_to_bq()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
